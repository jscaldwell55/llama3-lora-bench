{
  "config": {
    "name": "quick_test",
    "description": "Quick test configuration",
    "lora": {
      "r": 8,
      "lora_alpha": 16,
      "lora_dropout": 0.1,
      "target_modules": [
        "q_proj",
        "v_proj"
      ]
    },
    "training": {
      "batch_size": 2,
      "gradient_accumulation_steps": 2,
      "num_epochs": 1,
      "learning_rate": "3e-4",
      "warmup_steps": 10
    },
    "dataset": {
      "name": "tatsu-lab/alpaca",
      "max_samples": 100
    },
    "use_wandb": false
  },
  "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
  "start_time": "2025-07-04T22:07:43.958553",
  "model_load_time": 3.236341953277588,
  "gpu_available": "NVIDIA A100-SXM4-40GB",
  "training_time": 7.9490625858306885,
  "final_loss": 1.1917354202270507,
  "trainable_parameters": 1126400,
  "total_parameters": 1101174784,
  "peak_gpu_memory_gb": 2.6006102561950684,
  "samples_per_second": 12.580099718707883
}