dataset:
  max_samples: 500
lora:
  lora_alpha: 8
  lora_dropout: 0.05
  r: 4
  target_modules:
  - q_proj
  - v_proj
name: tiny_r4
training:
  batch_size: 4
  gradient_accumulation_steps: 2
  learning_rate: 3e-4
  num_epochs: 1
  warmup_steps: 20
